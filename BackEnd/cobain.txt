# # backend/model.py
# # Author: Elsie
# # Date: 2025
# # Description: Integrates Qwen-1.8B-Chat LLM for automated HSK essay scoring.
# #              Incorporates Jieba for preprocessing and enhanced prompt tuning.
# #              Enhanced with flexible input handling and cultural adaptation.

# from transformers import AutoModelForCausalLM, AutoTokenizer
# import json
# import re
# import logging
# import jieba.posseg as pseg
# import jieba
# from typing import Dict, List, Any, Optional
# import torch

# # Configure logger for this module
# logger = logging.getLogger(__name__)
# logging.basicConfig(level=logging.INFO)

# class QwenScorer:
#     """
#     Enhanced wrapper for the Qwen-1.8B-Chat model to score HSK Mandarin essays
#     written by Indonesian learners, focusing on culturally specific errors.
#     Integrates Jieba for preprocessing and flexible input handling.
#     """

#     def __init__(self, model_path: str = "Qwen/Qwen-1_8B-Chat"):
#         """
#         Initializes the QwenScorer by loading the model and tokenizer.
#         """
#         logger.info("Loading Qwen-1.8B-Chat...")
#         try:
#             self.tokenizer = AutoTokenizer.from_pretrained(
#                 model_path,
#                 trust_remote_code=True,
#                 padding_side='left'
#             )
#             logger.info("Tokenizer loaded successfully.")

#             self.model = AutoModelForCausalLM.from_pretrained(
#                 model_path,
#                 device_map="auto",
#                 trust_remote_code=True,
#                 torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
#                 low_cpu_mem_usage=True
#             ).eval()
#             logger.info("Qwen-1.8B-Chat model loaded and set to evaluation mode successfully.")
            
#             # Initialize Jieba with enhanced dictionary for Indonesian learner errors
#             jieba.initialize()
#             self._setup_cultural_patterns()
#             logger.info("Jieba library initialized with cultural patterns.")
            
#         except Exception as e:
#             logger.error(f"Failed to load Qwen-1.8B-Chat model or tokenizer: {e}")
#             raise

#     def _setup_cultural_patterns(self):
#         """Setup cultural adaptation patterns for Indonesian learners"""
#         self.cultural_errors = {
#             "word_order_errors": [
#                 (r"(.+)([吃喝学习工作])(.+)在", "S-P-O-K interference"),
#                 (r"我(吃饭|睡觉|学习)(.+)在", "Verb placement error"),
#             ],
#             "false_friends": {
#                 "忙": {"correct": "拥挤", "context": "for objects/places"},
#                 "大": {"correct": "年长", "context": "for age"}, 
#                 "小": {"correct": "年轻", "context": "for age"},
#                 "路忙": {"correct": "路拥挤", "context": "busy road"}
#             },
#             "particle_errors": [
#                 (r"是\d+岁", "Unnecessary 是 before age"),
#                 (r"的得?地?", "Particle confusion"),
#             ]
#         }

#     def preprocess_essay(self, essay: str) -> Dict[str, Any]:
#         """
#         Enhanced preprocessing with cultural error detection
#         """
#         # Clean and normalize text
#         cleaned_essay = self._clean_text(essay)
        
#         # Jieba segmentation with POS tagging
#         try:
#             words_with_pos = list(pseg.cut(cleaned_essay))
#             segments = [word for word, flag in words_with_pos]
#             pos_tags = [(word, flag) for word, flag in words_with_pos]
            
#             # Basic text statistics
#             stats = {
#                 "char_count": len(cleaned_essay),
#                 "word_count": len(segments),
#                 "sentence_count": len(re.split(r'[。！？]', cleaned_essay)) - 1,
#                 "avg_sentence_length": len(segments) / max(1, len(re.split(r'[。！？]', cleaned_essay)) - 1)
#             }
            
#             # Cultural error pre-detection
#             cultural_insights = self._pre_detect_cultural_errors(cleaned_essay)
            
#             return {
#                 "cleaned_text": cleaned_essay,
#                 "segments": segments,
#                 "pos_tags": pos_tags,
#                 "statistics": stats,
#                 "cultural_insights": cultural_insights
#             }
            
#         except Exception as e:
#             logger.error(f"Jieba preprocessing failed: {e}")
#             return {
#                 "cleaned_text": cleaned_essay,
#                 "segments": [cleaned_essay],
#                 "pos_tags": [],
#                 "statistics": {"char_count": len(cleaned_essay), "word_count": 1, "sentence_count": 1, "avg_sentence_length": 1},
#                 "cultural_insights": []
#             }

#     def _clean_text(self, text: str) -> str:
#         """Clean and normalize input text"""
#         # Remove extra whitespace but preserve Chinese punctuation
#         cleaned = re.sub(r'[ \t]+', '', text.strip())
#         # Ensure proper punctuation spacing
#         cleaned = re.sub(r'([。！？])([^」）])', r'\1\2', cleaned)
#         return cleaned

#     def _pre_detect_cultural_errors(self, text: str) -> List[Dict]:
#         """Pre-detect common cultural errors for Indonesian learners"""
#         insights = []
        
#         # Word order errors
#         for pattern, error_type in self.cultural_errors["word_order_errors"]:
#             matches = re.finditer(pattern, text)
#             for match in matches:
#                 insights.append({
#                     "type": "word_order",
#                     "fragment": match.group(0),
#                     "position": [match.start(), match.end()],
#                     "description": f"Potential {error_type}"
#                 })
        
#         # False friends
#         for false_friend, info in self.cultural_errors["false_friends"].items():
#             if false_friend in text:
#                 insights.append({
#                     "type": "false_friend",
#                     "fragment": false_friend,
#                     "suggestion": info["correct"],
#                     "description": f"False friend: {info['context']}"
#                 })
        
#         return insights

#     def _extract_json_from_response(self, response: str) -> Optional[str]:
#         """
#         Robust JSON extraction that handles incomplete JSON and markdown formatting.
#         """
#         if not response:
#             return None
            
#         # Remove markdown code blocks if present
#         cleaned_response = re.sub(r'^```json\s*|\s*```$', '', response.strip(), flags=re.MULTILINE)
        
#         # Find the first { and last }
#         start_idx = cleaned_response.find('{')
#         end_idx = cleaned_response.rfind('}')
        
#         if start_idx == -1 or end_idx == -1 or end_idx <= start_idx:
#             logger.error("No valid JSON structure found in response")
#             return None
            
#         json_str = cleaned_response[start_idx:end_idx+1]
        
#         # Try to parse and validate JSON
#         try:
#             parsed = json.loads(json_str)
#             logger.info("Successfully extracted and parsed JSON")
#             return json_str
#         except json.JSONDecodeError as e:
#             logger.warning(f"Initial JSON parsing failed: {e}. Attempting repair...")
#             return self._repair_incomplete_json(json_str)

#     def _repair_incomplete_json(self, incomplete_json: str) -> str:
#         """
#         Attempt to repair incomplete JSON by closing open structures.
#         """
#         try:
#             # Count open braces and brackets
#             open_braces = incomplete_json.count('{') - incomplete_json.count('}')
#             open_brackets = incomplete_json.count('[') - incomplete_json.count(']')
#             open_strings = incomplete_json.count('"') % 2
            
#             repaired = incomplete_json
            
#             # Close open strings
#             if open_strings:
#                 repaired += '"'
                
#             # Close open arrays first
#             for _ in range(open_brackets):
#                 repaired += ']'
                
#             # Close open objects
#             for _ in range(open_braces):
#                 repaired += '}'
            
#             # Final validation
#             parsed = json.loads(repaired)
#             logger.info("Successfully repaired incomplete JSON")
#             return repaired
            
#         except json.JSONDecodeError:
#             logger.error("Failed to repair JSON, returning minimal valid structure")
#             return json.dumps({
#                 "overall_score": 0,
#                 "detailed_scores": {
#                     "grammar": 0,
#                     "vocabulary": 0, 
#                     "coherence": 0,
#                     "cultural_adaptation": 0
#                 },
#                 "error_list": [],
#                 "feedback": "System temporarily unavailable. Please try again."
#             })

#     def _validate_and_correct_errors(self, error_list: list, essay: str) -> list:
#         """
#         Enhanced error validation with position correction and cultural focus.
#         """
#         validated_errors = []
        
#         for error in error_list:
#             if not isinstance(error, dict):
#                 continue
                
#             incorrect_frag = error.get("incorrect_fragment", "")
#             error_pos = error.get("error_position", [])
#             error_type = error.get("error_type", "")
            
#             # Skip if no fragment or invalid position
#             if not incorrect_frag or not isinstance(error_pos, list) or len(error_pos) != 2:
#                 continue
            
#             start, end = error_pos
            
#             # Validate bounds and content match
#             if (0 <= start < end <= len(essay)) and essay[start:end] == incorrect_frag:
#                 validated_errors.append(error)
#             else:
#                 # Try to find the fragment in the essay
#                 found_pos = essay.find(incorrect_frag)
#                 if found_pos != -1:
#                     corrected_error = error.copy()
#                     corrected_error["error_position"] = [found_pos, found_pos + len(incorrect_frag)]
#                     validated_errors.append(corrected_error)
#                     logger.info(f"Corrected error position for '{incorrect_frag}'")
#                 else:
#                     logger.warning(f"Hallucinated error removed: '{incorrect_frag}'")
        
#         return validated_errors

#     def _ensure_english_feedback(self, feedback: str) -> str:
#         """
#         Ensure feedback is in English with cultural sensitivity.
#         """
#         if not feedback or feedback.strip() in ["Tidak ada umpan balik.", "No feedback."]:
#             return "Good effort! Continue practicing your Mandarin writing skills."
        
#         # Detect non-English text (Chinese/Indonesian)
#         if re.search(r'[\u4e00-\u9fff]', feedback) or any(word in feedback.lower() for word in ['indonesia', 'bahasa']):
#             logger.warning("Non-English feedback detected, providing generic English feedback")
#             return "Thank you for your submission. Focus on the specific errors highlighted above to improve your Mandarin writing. Pay special attention to word order and particle usage common for Indonesian learners."
        
#         return feedback.strip()

#     def _build_cultural_prompt_context(self, cultural_insights: List[Dict]) -> str:
#         """Build context about detected cultural errors for the prompt"""
#         if not cultural_insights:
#             return ""
        
#         context = "\n**CULTURAL INSIGHTS (Pre-detected):**\n"
#         for insight in cultural_insights[:3]:  # Limit to top 3 insights
#             context += f"- {insight['type']}: {insight.get('fragment', '')} -> {insight.get('suggestion', 'Check usage')}\n"
        
#         return context


#     def generate_json(self, essay: str, hsk_level: int = 2) -> str:
#         """
#         Enhanced JSON generation with flexible input handling and cultural adaptation.
#         """
#         logger.info(f"Scoring essay (HSK {hsk_level}, length: {len(essay)} chars)")

#         # Enhanced preprocessing
#         preprocessed = self.preprocess_essay(essay)
#         cultural_context = self._build_cultural_prompt_context(preprocessed["cultural_insights"])
        
#         # Dynamic HSK rubrics based on text characteristics
#         actual_level = self._estimate_actual_level(preprocessed["statistics"], hsk_level)
#         rubric_description = self._get_rubric_description(actual_level)

#         # Enhanced prompt with cultural adaptation
#         prompt = self._build_enhanced_prompt(
#             essay=preprocessed["cleaned_text"],
#             hsk_level=actual_level,
#             rubric_description=rubric_description,
#             cultural_context=cultural_context,
#             statistics=preprocessed["statistics"]
#         )

#         try:
#             logger.info("Sending enhanced prompt to Qwen-1.8B-Chat...")
#             response, _ = self.model.chat(self.tokenizer, prompt, history=None)
#             logger.info("Received LLM response")

#             # Extract and process JSON
#             json_str = self._extract_json_from_response(response)
            
#             if not json_str:
#                 return self._create_fallback_response(essay, "No valid JSON response from model")

#             # Parse and validate response
#             final_result = self._process_and_validate_response(json_str, essay, preprocessed)
#             final_json = json.dumps(final_result, ensure_ascii=False, indent=2)
            
#             logger.info("Enhanced JSON generation completed successfully")
#             return final_json

#         except Exception as e:
#             logger.error(f"LLM processing failed: {e}")
#             return self._create_fallback_response(essay, f"Processing error: {str(e)}")

#     def _estimate_actual_level(self, statistics: Dict, requested_level: int) -> int:
#         """Dynamically estimate appropriate HSK level based on text characteristics"""
#         char_count = statistics["char_count"]
#         sentence_count = statistics["sentence_count"]
#         avg_length = statistics["avg_sentence_length"]
        
#         # Simple heuristic based on text characteristics
#         if char_count < 30 and sentence_count <= 3:
#             return min(requested_level, 1)
#         elif char_count < 100 and avg_length < 8:
#             return min(requested_level, 2)
#         else:
#             return min(requested_level, 3)

#     def _get_rubric_description(self, hsk_level: int) -> str:
#         """Get appropriate rubric description for HSK level"""
#         rubrics = {
#             1: "HSK 1: 10-30 characters, basic S-P or S-P-O structures, essential vocabulary only.",
#             2: "HSK 2: 30-100 characters, simple connectors (在, 和, 然后), basic sentence patterns.", 
#             3: "HSK 3: 100-200 characters, particles (的/得/地), complex sentences, expanded vocabulary."
#         }
#         return rubrics.get(hsk_level, rubrics[2])
#     def _build_enhanced_prompt(self, essay: str, hsk_level: int, rubric_description: str, 
#                              cultural_context: str, statistics: Dict) -> str:        # --- ULTIMATELY STRICT PROMPT ENGINEERING ---
#         # Force the LLM to analyze the specific input essay.
#         prompt = f"""
# You are an AUTOMATED HSK MANDARIN ESSAY SCORING SYSTEM for Indonesian learners.
# Your ONLY job is to analyze the essay and return a SINGLE, VALID JSON object.
# DO NOT provide any text, explanations, markdown, or greetings outside the JSON.

# INPUT ESSAY (Analyze this text ONLY):
# {essay}

# TASK:
# Fill the following PRECISE JSON TEMPLATE with your assessment based on HSK {hsk_level} criteria ({rubric_description}).
# Focus on detecting the THREE specific error types common to Indonesian learners:
# 1. S-P-O-K Interference (e.g., '我吃饭在餐厅' -> '我在餐厅吃饭')
# 2. False Friends (e.g., '路忙' -> '路拥挤')
# 3. Particle Misuse (e.g., '我妹妹是十岁' -> '我妹妹十岁')

# CRITICAL INSTRUCTIONS:
# - Analyze ONLY the provided 'INPUT ESSAY' character by character.
# - For EACH identified error of the three types listed above:
#    a. Identify the EXACT SUBSTRING from the 'INPUT ESSAY' that is incorrect. This is `incorrect_fragment`.
#    b. Determine the ZERO-BASED starting character index (inclusive) and the ending character index (exclusive) of this substring within the 'INPUT ESSAY'. Python slicing `essay[start:end]` should yield the `incorrect_fragment`. This is `error_position`.
#    c. Provide the precise correction for that substring IN CHINESE CHARACTERS (Hanzi). This is `suggested_correction`.
#    d. Classify the error type (`word_order`, `false_friend`, `particle_misuse`).
#    e. Write a concise explanation IN ENGLISH explaining WHY the original fragment was incorrect and how the correction fixes it. This is `explanation`.
# - CRITICAL: If NO ERRORS of the specified critical types are found, return an empty `error_list`: [].
# - CRITICAL: DO NOT invent or hallucinate errors that are not present in the 'INPUT ESSAY'.
# - CRITICAL: DO NOT make up scores. Assign realistic scores based on the actual quality of the essay.
# - CRITICAL: DO NOT provide feedback in Chinese. ALL feedback and explanations MUST BE IN ENGLISH.
# - CRITICAL: Respond ONLY with the SINGLE JSON object. NO OTHER TEXT, MARKDOWN, OR EXPLANATIONS OUTSIDE THE JSON.
# - CRITICAL: ALL JSON KEYS MUST BE IN `snake_case` (e.g., `overall_score`, `error_type`, `error_position`, `incorrect_fragment`, `suggested_correction`, `explanation`, `detailed_scores`, `cultural_adaptation`).
# - CRITICAL: `error_position` MUST BE an array of TWO INTEGERS: [start_character_index, end_character_index], derived from the 'INPUT ESSAY' text.
# - CRITICAL: `error_list` MUST BE an array, which can be empty [].
# - CRITICAL: `detailed_scores` MUST CONTAIN keys: `grammar`, `vocabulary`, `coherence`, `cultural_adaptation`.
# - CRITICAL: Corrections (`suggested_correction`) MUST BE IN CHINESE CHARACTERS (Hanzi).
# - CRITICAL: Explanations and general feedback MUST BE IN ENGLISH.

# EXAMPLE OF A GOOD RESPONSE (DO NOT COPY THIS EXACTLY, JUST FOLLOW THE FORMAT):
# {{
#   "overall_score": 85,
#   "detailed_scores": {{
#     "grammar": 90,
#     "vocabulary": 80,
#     "coherence": 85,
#     "cultural_adaptation": 90
#   }},
#   "error_list": [
#     {{
#       "error_type": "word_order",
#       "error_position": [4, 6],
#       "incorrect_fragment": "吃饭在",
#       "suggested_correction": "在吃饭",
#       "explanation": "Struktur S-P-O-K dari Bahasa Indonesia salah. Gunakan S-K-P-O: '我在餐厅吃饭'."
#     }}
#   ],
#   "feedback": "Esai sudah baik, tapi perhatikan urutan kata dan partikel."
# }}

# EXAMPLE OF A RESPONSE WITH NO ERRORS (DO NOT COPY THIS EXACTLY, JUST FOLLOW THE FORMAT):
# {{
#   "overall_score": 95,
#   "detailed_scores": {{
#     "grammar": 95,
#     "vocabulary": 90,
#     "coherence": 95,
#     "cultural_adaptation": 95
#   }},
#   "error_list": [],
#   "feedback": "Esai ini sangat baik. Struktur kalimat, tata bahasa, dan kosakata sudah sesuai dengan level HSK. Pertahankan!"
# }}

# INPUT ESSAY AGAIN (FOR CLARITY):
# {essay}

# PRECISE JSON TEMPLATE TO FILL (Return ONLY this, with filled values):
# {{
#   "overall_score": 0,
#   "detailed_scores": {{
#     "grammar": 0,
#     "vocabulary": 0,
#     "coherence": 0,
#     "cultural_adaptation": 0
#   }},
#   "error_list": [
#     {{
#       "error_type": "word_order",
#       "error_position": [0, 0],
#       "incorrect_fragment": "",
#       "suggested_correction": "",
#       "explanation": ""
#     }}
#   ],
#   "feedback": ""
# }}
# """

#         return prompt

    
#         # --- TAMBAHKAN METODE-METODE INI KE DALAM KELAS QwenScorer ---

#     def _validate_score(self, score: Any) -> int:
#         """
#         Validates and normalizes a score to be an integer between 0 and 100.
#         """
#         try:
#             # Convert to int first, handling floats or string numbers
#             score_val = int(score)
#             # Clamp the score between 0 and 100
#             return max(0, min(100, score_val))
#         except (ValueError, TypeError):
#             # If conversion fails, log a warning and return 0
#             logger.warning(f"Invalid score value '{score}' provided. Setting to 0.")
#             return 0

#     def _standardize_detailed_scores(self, raw_scores: Dict) -> Dict[str, int]:
#         """
#         Standardizes the detailed scores dictionary with flexible key matching.
#         Ensures all required keys are present and have valid integer values.
#         """
#         # Define mappings for different possible key names from LLM
#         score_mappings = {
#             "grammar": ["grammar", "Grammar", "tata_bahasa", "语法"],
#             "vocabulary": ["vocabulary", "Vocabulary", "kosa_kata", "词汇"], 
#             "coherence": ["coherence", "Coherence", "koherensi", "连贯性"],
#             "cultural_adaptation": ["cultural_adaptation", "Cultural Adaptation", 
#                                   "adaptasi_budaya", "文化适应", "cultural_adaptation"]
#         }
        
#         standardized = {}
#         for target_key, possible_keys in score_mappings.items():
#             score = 0
#             # Try to find the score using any of the possible key names
#             for key in possible_keys:
#                 if key in raw_scores and raw_scores[key] is not None:
#                     try:
#                         # Validate and normalize the score
#                         score = self._validate_score(raw_scores[key])
#                         break # Stop if a valid score is found
#                     except (ValueError, TypeError):
#                         # If validation fails for this key, try the next one
#                         continue
#             # Assign the found or default score
#             standardized[target_key] = score
            
#         return standardized

#     def _create_fallback_response(self, essay: str, error_msg: str) -> str:
#         """
#         Creates a fallback JSON response string when LLM processing fails.
#         This ensures the backend API always returns a valid JSON string.
#         """
#         fallback = {
#             "text": essay, # Always include the original text
#             "overall_score": 0, # Default score
#             "detailed_scores": {
#                 "grammar": 0,       # Default detailed scores
#                 "vocabulary": 0,
#                 "coherence": 0,
#                 "cultural_adaptation": 0
#             },
#             "error_list": [], # Empty error list
#             "feedback": "System temporarily unavailable. Please try again with a different essay.", # Generic feedback
#             "error": error_msg, # Include the specific error message for debugging
#             "metadata": {
#                 "fallback_used": True, # Flag to indicate this is a fallback
#                 "timestamp": "2025-10-19" # You can use datetime.datetime.now() for dynamic timestamp
#             }
#         }
#         # Convert the fallback dictionary to a JSON string and return it
#         # ensure_ascii=False is crucial for preserving Hanzi
#         return json.dumps(fallback, ensure_ascii=False, indent=2) 


#     def _process_and_validate_response(self, json_str: str, original_essay: str, preprocessed: Dict) -> Dict:
#         """Process and validate the model response, including strict hallucination check"""
#         try:
#             parsed_data = json.loads(json_str)
#         except json.JSONDecodeError as e:
#             logger.error(f"JSON parsing failed: {e}")
#             return self._create_fallback_response(original_essay, "Invalid JSON response")

#         # Validate and standardize scores
#         overall_score = self._validate_score(parsed_data.get("overall_score", 0))
        
#         detailed_scores = self._standardize_detailed_scores(
#             parsed_data.get("detailed_scores", {})
#         )

#         # --- ULTRA-STRICT ERROR LIST VALIDATION & SANITIZATION ---
#         # Ensure `error_list` items are dictionaries with required keys and types
#         # AND MOST IMPORTANTLY, validate that errors actually exist in the input essay.
#         raw_error_list = parsed_data.get("error_list", [])
#         validated_error_list = []
#         sanitized_count = 0 # Counter for removed hallucinations
#         if isinstance(raw_error_list, list):
#             for i, error_item in enumerate(raw_error_list):
#                 if isinstance(error_item, dict):
#                     # --- CORE VALIDATION LOGIC ---
#                     # 1. Get fields
#                     incorrect_frag = error_item.get("incorrect_fragment", "")
#                     error_pos = error_item.get("error_position", [])
#                     error_type = error_item.get("error_type", "unknown_error")

#                     # 2. Check if fragment and position are valid strings/lists
#                     if not isinstance(incorrect_frag, str) or not isinstance(error_pos, list) or len(error_pos) != 2:
#                         logger.warning(f"Invalid error item structure in error_list[{i}]. Removing.")
#                         sanitized_count += 1
#                         continue # Skip invalid item

#                     start_pos, end_pos = error_pos
#                     # 3. Check if positions are valid integers within essay bounds
#                     if not isinstance(start_pos, int) or not isinstance(end_pos, int) or start_pos < 0 or end_pos > len(original_essay) or start_pos >= end_pos:
#                          logger.warning(f"Invalid error position {error_pos} in error_list[{i}] for essay length {len(original_essay)}. Removing.")
#                          sanitized_count += 1
#                          continue # Skip item with invalid position

#                     # 4. CRITICAL CHECK: Does the fragment at the given position match the claimed incorrect fragment?
#                     extracted_from_essay = original_essay[start_pos:end_pos]
#                     if extracted_from_essay != incorrect_frag:
#                         logger.warning(f"MISMATCH! LLM claims error '{incorrect_frag}' at {error_pos}, but essay[{start_pos}:{end_pos}] is '{extracted_from_essay}'. This is a hallucination. Removing error_list[{i}].")
#                         sanitized_count += 1
#                         # Optionally, you could add a placeholder error indicating a hallucination was detected
#                         # validated_error_list.append({
#                         #     "error_type": "hallucination",
#                         #     "error_position": error_pos,
#                         #     "incorrect_fragment": incorrect_frag,
#                         #     "suggested_correction": "N/A",
#                         #     "explanation": f"System detected a discrepancy: LLM reported '{incorrect_frag}' at position {error_pos}, but the actual text was '{extracted_from_essay}'. This error might be fabricated."
#                         # })
#                         continue # Remove the hallucinated error

#                     # --- If validation passes, keep the error item ---
#                     validated_error_list.append(error_item)
#                     logger.info(f"Validated genuine error at position {error_pos}: '{incorrect_frag}'")

#                 else:
#                     logger.warning(f"Non-dictionary item found in error_list at index {i}. Skipping.")
#                     sanitized_count += 1
#             if sanitized_count > 0:
#                 logger.info(f"Sanitized {sanitized_count} hallucinated or invalid errors from LLM output.")
#         else:
#              logger.warning("`error_list` is not an array. Setting to empty array.")
#              validated_error_list = []

#         # Ensure `error_list` items are dictionaries with required keys and types
#         # AND MOST IMPORTANTLY, validate that errors actually exist in the input essay.
#         raw_error_list = parsed_data.get("error_list", [])
#         validated_error_list = []
#         sanitized_count = 0 # Counter for removed hallucinations
#         if isinstance(raw_error_list, list):
#             for i, error_item in enumerate(raw_error_list):
#                 if isinstance(error_item, dict):
#                     # --- CORE VALIDATION LOGIC ---
#                     # 1. Get fields
#                     incorrect_frag = error_item.get("incorrect_fragment", "")
#                     error_pos = error_item.get("error_position", [])
#                     error_type = error_item.get("error_type", "unknown_error")

#                     # 2. Check if fragment and position are valid strings/lists
#                     if not isinstance(incorrect_frag, str) or not isinstance(error_pos, list) or len(error_pos) != 2:
#                         logger.warning(f"Invalid error item structure in error_list[{i}]. Removing.")
#                         sanitized_count += 1
#                         continue # Skip invalid item

#                     start_pos, end_pos = error_pos
#                     # 3. Check if positions are valid integers within essay bounds
#                     if not isinstance(start_pos, int) or not isinstance(end_pos, int) or start_pos < 0 or end_pos > len(original_essay) or start_pos >= end_pos:
#                          logger.warning(f"Invalid error position {error_pos} in error_list[{i}] for essay length {len(original_essay)}. Removing.")
#                          sanitized_count += 1
#                          continue # Skip item with invalid position

#                     # 4. CRITICAL CHECK: Does the fragment at the given position match the claimed incorrect fragment?
#                     extracted_from_essay = original_essay[start_pos:end_pos]
#                     if extracted_from_essay != incorrect_frag:
#                         logger.warning(f"MISMATCH! LLM claims error '{incorrect_frag}' at {error_pos}, but essay[{start_pos}:{end_pos}] is '{extracted_from_essay}'. This is a hallucination. Removing error_list[{i}].")
#                         sanitized_count += 1
#                         # Optionally, you could add a placeholder error indicating a hallucination was detected
#                         # validated_error_list.append({
#                         #     "error_type": "hallucination",
#                         #     "error_position": error_pos,
#                         #     "incorrect_fragment": incorrect_frag,
#                         #     "suggested_correction": "N/A",
#                         #     "explanation": f"System detected a discrepancy: LLM reported '{incorrect_frag}' at position {error_pos}, but the actual text was '{extracted_from_essay}'. This error might be fabricated."
#                         # })
#                         continue # Remove the hallucinated error

#                     # --- If validation passes, keep the error item ---
#                     validated_error_list.append(error_item)
#                     logger.info(f"Validated genuine error at position {error_pos}: '{incorrect_frag}'")

#                 else:
#                     logger.warning(f"Non-dictionary item found in error_list at index {i}. Skipping.")
#                     sanitized_count += 1
#             if sanitized_count > 0:
#                 logger.info(f"Sanitized {sanitized_count} hallucinated or invalid errors from LLM output.")
#         else:
#              logger.warning("`error_list` is not an array. Setting to empty array.")
#              validated_error_list = []


#         # Ensure `feedback` is a string
#         feedback = parsed_data.get("feedback", "")
#         if not isinstance(feedback, str):
#              feedback = "Tidak ada umpan balik."

#         # Build final result with enhanced information
#         final_result = {
#             "text": original_essay, # Always include the original text for highlighting in frontend
#             "overall_score": overall_score,
#             "detailed_scores": detailed_scores,
#             "error_list": validated_error_list, # Use the validated and sanitized error list
#             "feedback": feedback,
#         }

#         return final_result
# backend/model.py
# Author: Elsie
# Date: 2025
# Description: Integrates Qwen-1.8B-Chat LLM for automated HSK essay scoring.
#              Incorporates Jieba for preprocessing and enhanced prompt tuning.

from transformers import AutoModelForCausalLM, AutoTokenizer
import json
import re
import logging

# Configure logger for this module
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO) # Adjust level as needed (DEBUG, INFO, WARNING, ERROR)

class QwenScorer:
    """
    Wrapper for the Qwen-1.8B-Chat model to score HSK Mandarin essays
    written by Indonesian learners, focusing on culturally specific errors.
    Integrates Jieba for preprocessing.
    """

    def __init__(self):
        """
        Initializes the QwenScorer by loading the model and tokenizer.
        """
        logger.info("Loading Qwen-1.8B-Chat...")
        try:
            # Load Tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                "Qwen/Qwen-1_8B-Chat", # Use forward slash for path consistency
                trust_remote_code=True
            )
            logger.info("Tokenizer loaded successfully.")

            # Load Model
            self.model = AutoModelForCausalLM.from_pretrained(
                "Qwen/Qwen-1_8B-Chat",
                device_map="auto",       # Automatically distribute across available devices (GPU/CPU)
                trust_remote_code=True,
                torch_dtype="auto"       # Automatically infer the best dtype
            ).eval() # Set to evaluation mode for inference
            logger.info("Qwen-1.8B-Chat model loaded and set to evaluation mode successfully.")
            
        except Exception as e:
            logger.error(f"Failed to load Qwen-1.8B-Chat model or tokenizer: {e}")
            raise # Re-raise the exception to halt initialization if model fails to load

    def generate_json(self, essay: str, hsk_level: int) -> str:
        """
        Generates a structured JSON assessment of a Mandarin essay by an Indonesian learner,
        focusing on errors specific to their linguistic background.
        """
        logger.info(f"Initiating scoring for HSK {hsk_level} essay (length: {len(essay)} chars).")

        # Define HSK rubrics
        rubrics = {
            1: "HSK 1: 10-15 characters, S-P or S-P-O structures only.",
            2: "HSK 2: 20-40 characters, use 在, 和, simple connectors.",
            3: "HSK 3: 50-100 characters, use 的/得/地, complex sentences."
        }
        rubric_description = rubrics.get(hsk_level, rubrics[1])

        # --- ULTIMATE PROMPT TUNING ---
        # Force the LLM to return ONLY a valid JSON object.
        prompt = f"""
SYSTEM: You are an AUTOMATED HSK MANDARIN ESSAY SCORING SYSTEM for Indonesian learners.
SYSTEM: You MUST respond ONLY with a SINGLE, VALID JSON object.
SYSTEM: DO NOT provide any text, explanations, markdown, or greetings outside the JSON.
SYSTEM: Failure to comply will result in a system error.

INPUT ESSAY (HSK {hsk_level}):
{essay}

TASK:
1. Analyze the essay for grammar, vocabulary, coherence, and cultural adaptation errors SPECIFIC TO INDONESIAN LEARNERS.
2. Focus ONLY on these THREE error types:
   a. S-P-O-K Interference (e.g., '我吃饭在餐厅' -> '我在餐厅吃饭')
   b. False Friends (e.g., '路忙' -> '路拥挤')
   c. Particle Misuse (e.g., '我妹妹是十岁' -> '我妹妹十岁')
3. ASSIGN REALISTIC SCORES OUT OF 100. DO NOT GIVE SCORES LIKE 0, 1, 5, 9 UNLESS THE ESSAY IS TRULY TERRIBLE OR EMPTY.
4. ONLY REPORT ERRORS THAT ACTUALLY EXIST IN THE INPUT ESSAY.
5. DO NOT INVENT OR HALLUCINATE ERRORS.
6. ALL OUTPUT MUST BE IN ENGLISH EXCEPT FOR `suggested_correction` (which MUST BE IN CHINESE CHARACTERS).
7. `error_position` MUST BE [start_index, end_index] based on the INPUT ESSAY.

MANDATORY JSON TEMPLATE (Return ONLY this structure, filled with your analysis):
{{
  "overall_score": 85,
  "detailed_scores": {{
    "grammar": 90,
    "vocabulary": 80,
    "coherence": 85,
    "cultural_adaptation": 90
  }},
  "error_list": [
    {{
      "error_type": "word_order",
      "error_position": [4, 6],
      "incorrect_fragment": "吃饭在",
      "suggested_correction": "在吃饭",
      "explanation": "Struktur S-P-O-K dari Bahasa Indonesia salah. Gunakan S-K-P-O: '我在餐厅吃饭'."
    }}
  ],
  "feedback": "Esai sudah baik, tapi perhatikan urutan kata dan partikel."
}}

CRITICAL INSTRUCTIONS:
- Analyze ONLY the provided 'INPUT ESSAY' character by character.
- For EACH identified error of the three types listed above:
   a. Identify the EXACT SUBSTRING from the 'INPUT ESSAY' that is incorrect. This is `incorrect_fragment`.
   b. Determine the ZERO-BASED starting character index (inclusive) and the ending character index (exclusive) of this substring within the 'INPUT ESSAY'. Python slicing `essay[start:end]` should yield the `incorrect_fragment`. This is `error_position`.
   c. Provide the precise correction for that substring IN CHINESE CHARACTERS (Hanzi). This is `suggested_correction`.
   d. Classify the error type (`word_order`, `false_friend`, `particle_misuse`).
   e. Write a concise explanation IN ENGLISH explaining WHY the original fragment was incorrect and how the correction fixes it. This is `explanation`.
- CRITICAL: If NO ERRORS of the specified critical types are found, return an empty `error_list`: [].
- CRITICAL: DO NOT invent or hallucinate errors that are not present in the 'INPUT ESSAY'.
- CRITICAL: DO NOT make up scores. Assign realistic scores based on the actual quality of the essay.
- CRITICAL: DO NOT provide feedback in Chinese. ALL feedback and explanations MUST BE IN ENGLISH.
- CRITICAL: Respond ONLY with the SINGLE JSON object. NO OTHER TEXT, MARKDOWN, OR EXPLANATIONS OUTSIDE THE JSON.
- CRITICAL: ALL JSON KEYS MUST BE IN `snake_case` (e.g., `overall_score`, `error_type`, `error_position`, `incorrect_fragment`, `suggested_correction`, `explanation`, `detailed_scores`, `cultural_adaptation`).
- CRITICAL: `error_position` MUST BE an array of TWO INTEGERS: [start_character_index, end_character_index], derived from the 'INPUT ESSAY' text.
- CRITICAL: `error_list` MUST BE an array, which can be empty [].
- CRITICAL: `detailed_scores` MUST CONTAIN keys: `grammar`, `vocabulary`, `coherence`, `cultural_adaptation`.
- CRITICAL: Corrections (`suggested_correction`) MUST BE IN CHINESE CHARACTERS (Hanzi).
- CRITICAL: Explanations and general feedback MUST BE IN ENGLISH.

INPUT ESSAY AGAIN FOR CLARITY:
{essay}

FILL TEMPLATE NOW. OUTPUT ONLY THE JSON. NO OTHER TEXT.
"""

        try:
            logger.debug("Sending prompt to Qwen-1.8B-Chat...")
            # --- LLM INFERENCE ---
            response, _ = self.model.chat(self.tokenizer, prompt, history=None)
            logger.debug("Received raw LLM response.")

            # --- ULTRA-ROBUST JSON EXTRACTION & PARSING ---
            # 1. Strip leading/trailing whitespace
            stripped_response = response.strip()
            logger.debug(f"Stripped response length: {len(stripped_response)}")

            # 2. Aggressively remove Markdown code blocks if present
            if stripped_response.startswith("```json"):
                stripped_response = stripped_response[7:] # Remove ```json
            elif stripped_response.startswith("```"):
                stripped_response = stripped_response[3:] # Remove ```
            
            if stripped_response.endswith("```"):
                stripped_response = stripped_response[:-3] # Remove ```
            
            stripped_response = stripped_response.strip() # Strip again after removing wrappers

            # 3. Find the first '{{' and the last '}}'
            first_brace = stripped_response.find('{{')
            last_brace = stripped_response.rfind('}}')

            json_str = None
            if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
                # 4. Extract the substring that should be the JSON object
                json_str = stripped_response[first_brace:last_brace+2] # +2 to include both braces
                logger.info(f"Extracted potential JSON block (length: {len(json_str)} chars). First 200 chars: {json_str[:200]}...")
            else:
                # Fallback: Try to find any JSON-like structure
                fallback_match = re.search(r'\{.*\}', stripped_response, re.DOTALL)
                if fallback_match:
                    json_str = fallback_match.group(0)
                    logger.info(f"Fallback regex found JSON block (length: {len(json_str)} chars). First 200 chars: {json_str[:200]}...")
                else:
                    error_msg = "No JSON block found in LLM response."
                    logger.error(error_msg)
                    return json.dumps({"error": error_msg, "raw_response": response})

            # 5. Attempt to parse the extracted string
            if json_str:
                try:
                    # Basic cleaning: Remove common problematic control characters that can break JSON
                    cleaned_json_str = json_str.replace('\n', '').replace('\r', '').replace('\t', '')
                    # Final parsing attempt
                    parsed_data = json.loads(cleaned_json_str)
                    logger.info("Successfully parsed extracted JSON string.")
                    
                    # --- POST-PARSE VALIDATION & DEFAULTS ---
                    # Ensure the parsed data is a dictionary
                    if not isinstance(parsed_data, dict):
                        raise ValueError("Parsed JSON is not a dictionary/object.")

                    # Ensure required top-level keys exist with correct types or provide defaults
                    required_top_keys = ["overall_score", "detailed_scores", "error_list", "feedback"]
                    for key in required_top_keys:
                        if key not in parsed_data:
                            logger.warning(f"Missing key '{key}' in LLM output. Adding default.")
                            if key == "overall_score":
                                parsed_data[key] = 0
                            elif key == "detailed_scores":
                                parsed_data[key] = {"grammar": 0, "vocabulary": 0, "coherence": 0, "cultural_adaptation": 0}
                            elif key == "error_list":
                                parsed_data[key] = []
                            elif key == "feedback":
                                parsed_data[key] = "Tidak ada umpan balik."

                    # Validate types of top-level keys
                    if not isinstance(parsed_data.get("overall_score", None), (int, float)):
                        parsed_data["overall_score"] = 0
                    if not isinstance(parsed_data.get("detailed_scores", None), dict):
                        parsed_data["detailed_scores"] = {"grammar": 0, "vocabulary": 0, "coherence": 0, "cultural_adaptation": 0}
                    if not isinstance(parsed_data.get("error_list", None), list):
                         parsed_data["error_list"] = []
                    if not isinstance(parsed_data.get("feedback", None), str):
                         parsed_data["feedback"] = "Tidak ada umpan balik."

                    # --- FINAL STRUCTURE CHECK ---
                    # Ensure `detailed_scores` has all required keys
                    ds = parsed_data.get("detailed_scores", {})
                    required_ds_keys = ["grammar", "vocabulary", "coherence", "cultural_adaptation"]
                    for ds_key in required_ds_keys:
                        if ds_key not in ds or not isinstance(ds[ds_key], (int, float)):
                            logger.warning(f"Missing or invalid detailed score '{ds_key}'. Setting to 0.")
                            ds[ds_key] = 0
                    parsed_data["detailed_scores"] = ds

                    # --- ENHANCED ERROR LIST VALIDATION & SANITIZATION ---
                    # Ensure `error_list` items are dictionaries with required keys and types
                    # AND MOST IMPORTANTLY, validate that errors actually exist in the input essay.
                    el = parsed_data.get("error_list", [])
                    validated_el = []
                    sanitized_count = 0 # Counter for removed hallucinations
                    if isinstance(el, list):
                        for i, error_item in enumerate(el):
                            if isinstance(error_item, dict):
                                # --- CORE VALIDATION LOGIC ---
                                # 1. Get fields
                                incorrect_frag = error_item.get("incorrect_fragment", "")
                                error_pos = error_item.get("error_position", [])
                                error_type = error_item.get("error_type", "unknown_error")

                                # 2. Check if fragment and position are valid strings/lists
                                if not isinstance(incorrect_frag, str) or not isinstance(error_pos, list) or len(error_pos) != 2:
                                    logger.warning(f"Invalid error item structure in error_list[{i}]. Removing.")
                                    sanitized_count += 1
                                    continue # Skip invalid item

                                start_pos, end_pos = error_pos
                                # 3. Check if positions are valid integers within essay bounds
                                if not isinstance(start_pos, int) or not isinstance(end_pos, int) or start_pos < 0 or end_pos > len(essay) or start_pos >= end_pos:
                                     logger.warning(f"Invalid error position {error_pos} in error_list[{i}] for essay length {len(essay)}. Removing.")
                                     sanitized_count += 1
                                     continue # Skip item with invalid position

                                # 4. CRITICAL CHECK: Does the fragment at the given position match the claimed incorrect fragment?
                                extracted_from_essay = essay[start_pos:end_pos]
                                if extracted_from_essay != incorrect_frag:
                                    logger.warning(f"MISMATCH! LLM claims error '{incorrect_frag}' at {error_pos}, but essay[{start_pos}:{end_pos}] is '{extracted_from_essay}'. This is a hallucination. Removing error_list[{i}].")
                                    sanitized_count += 1
                                    # Optionally, you could add a placeholder error indicating a hallucination was detected
                                    # validated_el.append({
                                    #     "error_type": "hallucination",
                                    #     "error_position": error_pos,
                                    #     "incorrect_fragment": incorrect_frag,
                                    #     "suggested_correction": "N/A",
                                    #     "explanation": f"System detected a discrepancy: LLM reported '{incorrect_frag}' at position {error_pos}, but the actual text was '{extracted_from_essay}'. This error might be fabricated."
                                    # })
                                    continue # Remove the hallucinated error

                                # --- If validation passes, keep the error item ---
                                # (You can still do the key/type validation as before if needed, but it's less critical now)
                                validated_el.append(error_item)
                                logger.info(f"Validated genuine error at position {error_pos}: '{incorrect_frag}'")

                            else:
                                logger.warning(f"Non-dictionary item found in error_list at index {i}. Skipping.")
                                sanitized_count += 1
                        parsed_data["error_list"] = validated_el
                        if sanitized_count > 0:
                            logger.info(f"Sanitized {sanitized_count} hallucinated or invalid errors from LLM output.")
                    else:
                         logger.warning("`error_list` is not an array. Setting to empty array.")
                         parsed_data["error_list"] = []


                    # --- FINAL DATA STANDARDIZATION & FORMATTING ---
                    # This is the CRUCIAL step to ensure the output ALWAYS matches the frontend's expectations.
                    # Regardless of what the LLM returns, we build a new, clean object with the correct structure.
                    # This guarantees that main.py receives a consistent format.

                    # 1. Extract and standardize the overall score
                    overall_score_raw = parsed_data.get("overall_score") or parsed_data.get("Overall Score") or parsed_data.get("Overall_Score") or 0
                    # Clamp score between 0 and 100
                    standardized_overall_score = max(0, min(100, int(overall_score_raw)))

                    # 2. Extract and standardize detailed scores
                    raw_detailed_scores = parsed_data.get("detailed_scores") or parsed_data.get("Detailed Scores") or parsed_data.get("Detailed_Scores") or {}
                    standardized_detailed_scores = {
                        "grammar": max(0, min(100, int(
                            raw_detailed_scores.get("grammar") or
                            raw_detailed_scores.get("Grammar") or 0
                        ))),
                        "vocabulary": max(0, min(100, int(
                            raw_detailed_scores.get("vocabulary") or
                            raw_detailed_scores.get("Vocabulary") or 0
                        ))),
                        "coherence": max(0, min(100, int(
                            raw_detailed_scores.get("coherence") or
                            raw_detailed_scores.get("Coherence") or 0
                        ))),
                        "cultural_adaptation": max(0, min(100, int(
                            raw_detailed_scores.get("cultural_adaptation") or
                            raw_detailed_scores.get("Cultural Adaptation") or
                            raw_detailed_scores.get("Cultural_Adaptation") or 0
                        ))),
                    }

                    # 3. Use the validated error list
                    standardized_error_list = parsed_data.get("error_list", [])

                    # 4. Extract and standardize feedback
                    standardized_feedback = parsed_data.get("feedback") or parsed_data.get("Feedback") or "Tidak ada umpan balik."

                    # 5. Build the final, standardized result object
                    final_result = {
                        "text": essay, # Always include the original text for highlighting in frontend
                        "overall_score": standardized_overall_score,
                        "detailed_scores": standardized_detailed_scores,
                        "error_list": standardized_error_list,
                        "feedback": standardized_feedback,
                    }

                    # 6. Convert the final result object to a JSON string and return it
                    final_json_str = json.dumps(final_result, ensure_ascii=False) # ensure_ascii=False for Hanzi
                    logger.info("Final standardized JSON string generated successfully.")
                    return final_json_str # Return the clean, valid JSON string

                except json.JSONDecodeError as je:
                    logger.error(f"JSON Decode Error: {je}")
                    logger.error(f"Raw output was: {json_str if 'json_str' in locals() else 'N/A'}")
                    # Last resort: Try parsing the raw response if it looks like JSON
                    try:
                        parsed_data = json.loads(response)
                        logger.info("Parsed raw LLM response as a fallback.")
                        return json.dumps(parsed_data, ensure_ascii=False)
                    except json.JSONDecodeError:
                        pass # If raw response also fails, proceed to return error object

            # If all extraction and parsing attempts fail
            error_msg = f"Failed to decode valid JSON from extracted LLM response block."
            logger.error(error_msg)
            return json.dumps({"error": error_msg, "raw_response": response})

        except Exception as e:
            error_msg = f"LLM inference or processing failed unexpectedly: {repr(e)}"
            logger.exception(error_msg) # Logs the full traceback
            # Return a consistent error format for the backend API to handle
            return json.dumps({
                "error": error_msg,
                "raw_response": response if 'response' in locals() else ""
            })
